{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 â€“  Word Embeddings\n",
    "\n",
    "In this assignment, we are going to train a dynamic word embedding from scratch on newspaper data. The data is available [here](), and covers news articles between 2005 and 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the ```probabilistic_word_embeddings``` package. It can be installed from pypi. We also need ```networkx``` and  ```pandas``` , and a library for plotting, eg. ```seaborn```.\n",
    "\n",
    "The documentation for the PWE module is available [here](https://ninpnin.github.io/probabilistic-word-embeddings/probabilistic_word_embeddings.html). Moreover, an example of training a dynamic embedding (which might come in handy), is available [here](https://github.com/ninpnin/probabilistic-word-embeddings/blob/main/examples/dynamic.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install probabilistic_word_embeddings\n",
    "!pip install networkx pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import probabilistic_word_embeddings as pwe\n",
    "from probabilistic_word_embeddings.preprocessing import preprocess_standard, preprocess_partitioned\n",
    "from probabilistic_word_embeddings.embeddings import LaplacianEmbedding\n",
    "from probabilistic_word_embeddings.estimation import map_estimate\n",
    "from probabilistic_word_embeddings.evaluation import evaluate_on_holdout_set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing you want to do is read in the data (CSV), remove null rows, filter out languages other than english and take a subset of ~ 1000 rows for development. Come back and train with full data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to preprocess the data. First, save the contents of the Text column as a list, and split each article by whitespace to get a list of lists. Then, use the dynamic.py example as a reference on how to use the preprocess_partitioned function. Provide the years as labels. Use the ```downsample=False``` flag in the preprocessing function. After this, you should be left with the preprocessed articles, as well as the resulting vocabulary for the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"Text\"]\n",
    "texts = [t.split() for t in texts]\n",
    "labels = df[\"Year\"]\n",
    "texts, vocabulary = preprocess_partitioned # ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first and the last article to make sure they have been processed properly. You should see lists of words, eg. ```[\"the_2024\", \"dog_2024\"]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0], texts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are creating a dynamic model, we need a prior graph. Each word vector is connected to the same word vector for the previous and next year. For instance, $\\text{dog}_{2023}$ would be connected to $\\text{dog}_{2024}$. The dynamic.py example file is helpful when creating the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()\n",
    "for # ...:\n",
    "    # ...\n",
    "    g.add_edge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding Since "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = LaplacianEmbedding(vocabulary, graph=g, dimensionality=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the embedding using ```map_estimate```. Feel free to set model to sgns or cbow, window size ws to anything between 2 and 10. Other reasonable hyperparameters are epochs=10, batch_size=5000 or 10000. Finally, save the embedding using e.save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = map_estimate # ...\n",
    "e.save(\"embedding.pkl\") # if you train multiple models, save them under different names\n",
    "\n",
    "# load by:\n",
    "# e = LaplacianEmbedding(saved_model_path=\"embedding.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "At this stage, you want to analyze the word embeddings you have trained. Oftentimes, it is useful to know which words are similar to each other. In word embeddings, this can be done with cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the cosine similarity metric. It is defined as\n",
    "\n",
    "$$\n",
    "cossim(a, b) = \\frac{a \\cdot b}{\\lVert a\\rVert \\lVert b\\rVert}\n",
    "$$\n",
    "\n",
    "i.e. the dot product between the vectors $a$ and $b$, divided by the norm of $a$ and the norm of $b$. The dot product is available in as function in numpy; norm is available in numpy's linalg submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return # ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick two words from the same year (using the syntax ```e[\"dog_2024]```) and calculate their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = # ...\n",
    "similarity = cosine_similarity(e[w1], e[w2])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select another pair of words. Your task is to plot the similarity of the pair of words over time on a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the similarity for each year, and\n",
    "# use eg. Matplotlib : plt.plot()\n",
    "# or Seaborn : sns.lineplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to know what the semantically closest words are . There is a function for this, ```nearest_neighbors```. Use it to extract the 10 closest words to \"bread\", both in 2005 and 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probabilistic_word_embeddings.utils import nearest_neighbors\n",
    "\n",
    "w3 = # ...\n",
    "results = nearest_neighbors # ..\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
